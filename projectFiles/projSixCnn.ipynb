{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKuTiSPLjbBy"
   },
   "source": [
    "PreProcess Data (image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ySR-qRQSuEL",
    "outputId": "19f62186-f0af-47d9-bfdf-2e64abfe0446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyWavelets in /home/thebeast/anaconda3/lib/python3.7/site-packages (1.1.1)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/thebeast/anaconda3/lib/python3.7/site-packages (from PyWavelets) (1.21.6)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install PyWavelets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lsPladDJZsak",
    "outputId": "cc3f7b55-9af9-40f7-9f16-ddbf1128bb89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image file paths found: 684113\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imghdr\n",
    "import numpy as np\n",
    "from numpy.fft import fft2, fftshift\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from prnu import functions  # Ensure your 'prnu' module is available\n",
    "\n",
    "data_dir = 'images'\n",
    "image_exts = ['jpg', 'jpeg', 'png']\n",
    "levels = 4\n",
    "sigma = 5\n",
    "target_size = (128, 128)\n",
    "\n",
    "label_to_index = {\"real\": 0, \"fake\": 1}\n",
    "\n",
    "def list_image_files(data_dir, image_exts):\n",
    "    \"\"\"\n",
    "    Walks through subdirectories of data_dir and returns a list of file paths and corresponding numeric labels.\n",
    "    Expects folder names like \"0_real\" or \"1_fake\" (the label is taken as the part after the underscore).\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    for folder in os.listdir(data_dir):\n",
    "        folder_path = os.path.join(data_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        label_str = folder.split('_')[-1]\n",
    "        if label_str not in label_to_index:\n",
    "            continue\n",
    "        numeric_label = label_to_index[label_str]\n",
    "\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            file_paths.append(file_path)\n",
    "            labels.append(numeric_label)\n",
    "    return file_paths, labels\n",
    "\n",
    "all_file_paths, all_labels = list_image_files(data_dir, image_exts)\n",
    "print(\"Total image file paths found:\", len(all_file_paths))\n",
    "\n",
    "# Split into training and test sets\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(\n",
    "    all_file_paths, all_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iunjOt4gjgfD"
   },
   "source": [
    "Configure and Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 944
    },
    "id": "LEq3ztLt9dMv",
    "outputId": "202ba22c-2740-4822-b55e-6796e097359c"
   },
   "outputs": [],
   "source": [
    "def _load_and_process_image(file_path_str):\n",
    "    \"\"\"\n",
    "    Given a file path, load the image, process it to extract the PRNU noise residual,\n",
    "    apply Fourier transform and normalization, and return a  randomly cropped (128,128) float32 array.\n",
    "    \"\"\"\n",
    "    \n",
    "    img = cv2.imread(file_path_str, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image: {file_path_str}\")\n",
    "\n",
    "    # Extract the PRNU noise residual using PRNU module\n",
    "    prnu_noise = functions.extract_single(img, levels=levels, sigma=sigma)\n",
    "    \n",
    "    # Randomly crop a patch of size target_size from the noise residual\n",
    "    crop_h, crop_w = target_size\n",
    "    h, w = prnu_noise.shape\n",
    "    if h < crop_h or w < crop_w:\n",
    "        raise ValueError(f\"Image {file_path_str} is too small for target crop size {target_size}\")\n",
    "    \n",
    "    start_y = np.random.randint(0, h - crop_h + 1)\n",
    "    start_x = np.random.randint(0, w - crop_w + 1)\n",
    "    prnu_noise_cropped = prnu_noise[start_y:start_y + crop_h, start_x:start_x + crop_w]\n",
    "\n",
    "    # Compute Fourier transform and shift the zero frequency component to the center\n",
    "    prnu_noise_fourier = fft2(prnu_noise_cropped)\n",
    "    prnu_noise_fourier_shifted = fftshift(prnu_noise_fourier)\n",
    "    prnu_noise_fourier_magnitude = np.abs(prnu_noise_fourier_shifted)\n",
    "    \n",
    "    # Normalize the magnitude to the range [0, 1]\n",
    "    min_val = np.min(prnu_noise_fourier_magnitude)\n",
    "    max_val = np.max(prnu_noise_fourier_magnitude)\n",
    "    prnu_noise_normalized = (prnu_noise_fourier_magnitude - min_val) / (max_val - min_val + 1e-8)\n",
    "    \n",
    "    return prnu_noise_normalized.astype(np.float32)\n",
    "\n",
    "def _load_image_fn(fp):\n",
    "    \"\"\"\n",
    "    This function is wrapped by tf.py_function. It takes the file path input,\n",
    "    converts it to a Python string, and calls _load_and_process_image.\n",
    "    \"\"\"\n",
    "    # If fp is an EagerTensor, convert it to a numpy scalar.\n",
    "    if hasattr(fp, \"numpy\"):\n",
    "        fp = fp.numpy()\n",
    "    # fp is now expected to be of type bytes.\n",
    "    # Convert bytes to string if necessary.\n",
    "    file_path = fp.decode('utf-8') if isinstance(fp, bytes) else fp\n",
    "    return _load_and_process_image(file_path)\n",
    "\n",
    "def load_and_process_image(file_path):\n",
    "    \"\"\"\n",
    "    Wrap _load_image_fn with tf.py_function so that it can be used in the tf.data pipeline.\n",
    "    \"\"\"\n",
    "    image = tf.py_function(\n",
    "        func=_load_image_fn,\n",
    "        inp=[file_path],\n",
    "        Tout=tf.float32\n",
    "    )\n",
    "    # Set static shape: the image is target_size (e.g., (128, 128)).\n",
    "    image.set_shape(target_size)\n",
    "    # Add a channel dimension to form (128, 128, 1) for the CNN.\n",
    "    image = tf.expand_dims(image, axis=-1)\n",
    "    return image\n",
    "\n",
    "def process_path(file_path, label):\n",
    "    \"\"\"\n",
    "    Given a file path and label, load the image and return (image, label).\n",
    "    \"\"\"\n",
    "    image = load_and_process_image(file_path)\n",
    "    return image, label\n",
    "\n",
    "# Create training and test datasets from the file lists.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_files, test_labels))\n",
    "\n",
    "# Map the process_path function to load and process each image lazily.\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(buffer_size=1000)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.batch(batch_size)\n",
    "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arvbvZAMjkRU"
   },
   "source": [
    "Plot Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "A-R-xBrijZTQ",
    "outputId": "f1776114-16db-4d18-ba34-7653cc0f559f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 126, 126, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 63, 63, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 63, 63, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30, 30, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               2359552   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,747,649\n",
      "Trainable params: 2,747,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Epoch 1/100\n",
      "11257/17103 [==================>...........] - ETA: 1:08:56 - loss: 0.6932 - accuracy: 0.5008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thebeast/JunLeeCapstone/prnu/functions.py:340: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = x * noise_var / (coef_var_min + noise_var)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17103/17103 [==============================] - 14889s 869ms/step - loss: 0.6932 - accuracy: 0.5008 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 2/100\n",
      "17103/17103 [==============================] - 14421s 842ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 3/100\n",
      "17103/17103 [==============================] - 14485s 846ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 4/100\n",
      "17103/17103 [==============================] - 14453s 844ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 5/100\n",
      "17103/17103 [==============================] - 14454s 844ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 6/100\n",
      "17103/17103 [==============================] - 14619s 853ms/step - loss: 0.6932 - accuracy: 0.5005 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 7/100\n",
      "17103/17103 [==============================] - 14589s 852ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 8/100\n",
      "17103/17103 [==============================] - 14602s 853ms/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 9/100\n",
      "17103/17103 [==============================] - 14670s 856ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 10/100\n",
      "17103/17103 [==============================] - 14760s 862ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 11/100\n",
      "17103/17103 [==============================] - 14928s 872ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 12/100\n",
      "17103/17103 [==============================] - 14923s 871ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 13/100\n",
      "17103/17103 [==============================] - 14954s 873ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 14/100\n",
      "17103/17103 [==============================] - 14885s 869ms/step - loss: 0.6932 - accuracy: 0.5005 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 15/100\n",
      "17103/17103 [==============================] - 14897s 870ms/step - loss: 0.6932 - accuracy: 0.5006 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 16/100\n",
      "17103/17103 [==============================] - 14979s 875ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 17/100\n",
      "17103/17103 [==============================] - 14983s 875ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 18/100\n",
      "17103/17103 [==============================] - 14996s 876ms/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 19/100\n",
      "17103/17103 [==============================] - 15256s 891ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 20/100\n",
      "17103/17103 [==============================] - 15119s 883ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 21/100\n",
      "17103/17103 [==============================] - 15137s 884ms/step - loss: 0.6932 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 22/100\n",
      "17103/17103 [==============================] - 15088s 881ms/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 23/100\n",
      "17103/17103 [==============================] - 15133s 884ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 24/100\n",
      "17103/17103 [==============================] - 15125s 883ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 25/100\n",
      "17103/17103 [==============================] - 15103s 882ms/step - loss: 0.6932 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 26/100\n",
      "17103/17103 [==============================] - 15096s 881ms/step - loss: 0.6932 - accuracy: 0.4998 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 27/100\n",
      "17103/17103 [==============================] - 15120s 883ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 28/100\n",
      "17103/17103 [==============================] - 15127s 883ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 29/100\n",
      "17103/17103 [==============================] - 15135s 884ms/step - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 30/100\n",
      "17103/17103 [==============================] - 15145s 884ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 31/100\n",
      "17103/17103 [==============================] - 15129s 883ms/step - loss: 0.6932 - accuracy: 0.5008 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 32/100\n",
      "17103/17103 [==============================] - 15124s 883ms/step - loss: 0.6932 - accuracy: 0.4998 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 33/100\n",
      "17103/17103 [==============================] - 15114s 882ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 34/100\n",
      "17103/17103 [==============================] - 15135s 884ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 35/100\n",
      "17103/17103 [==============================] - 15155s 885ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 36/100\n",
      "17103/17103 [==============================] - 15154s 885ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 37/100\n",
      "17103/17103 [==============================] - 15176s 886ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 38/100\n",
      "17103/17103 [==============================] - 15153s 885ms/step - loss: 0.6932 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 39/100\n",
      "17103/17103 [==============================] - 15162s 885ms/step - loss: 0.6932 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 40/100\n",
      "17103/17103 [==============================] - 15146s 884ms/step - loss: 0.6932 - accuracy: 0.5009 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 41/100\n",
      "17103/17103 [==============================] - 15144s 884ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 42/100\n",
      "17103/17103 [==============================] - 15156s 885ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 43/100\n",
      "17103/17103 [==============================] - 15152s 885ms/step - loss: 0.6932 - accuracy: 0.5006 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 44/100\n",
      "17103/17103 [==============================] - 15166s 885ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 45/100\n",
      "17103/17103 [==============================] - 15137s 884ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 46/100\n",
      "17103/17103 [==============================] - 15164s 885ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 47/100\n",
      "17103/17103 [==============================] - 15163s 885ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 48/100\n",
      "17103/17103 [==============================] - 15131s 883ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 49/100\n",
      "17103/17103 [==============================] - 15165s 885ms/step - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 50/100\n",
      "17103/17103 [==============================] - 15172s 886ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 51/100\n",
      "17103/17103 [==============================] - 15185s 887ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 52/100\n",
      "17103/17103 [==============================] - 15144s 884ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 53/100\n",
      "17103/17103 [==============================] - 15159s 885ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17103/17103 [==============================] - 15066s 880ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 55/100\n",
      "17103/17103 [==============================] - 15073s 880ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 56/100\n",
      "17103/17103 [==============================] - 15081s 881ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 57/100\n",
      "17103/17103 [==============================] - 15051s 879ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 58/100\n",
      "17103/17103 [==============================] - 15053s 879ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 59/100\n",
      "17103/17103 [==============================] - 15046s 878ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 60/100\n",
      "17103/17103 [==============================] - 15039s 878ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 61/100\n",
      "17103/17103 [==============================] - 15037s 878ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 62/100\n",
      "17103/17103 [==============================] - 15119s 883ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 63/100\n",
      "17103/17103 [==============================] - 15047s 878ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 64/100\n",
      "17103/17103 [==============================] - 15101s 882ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 65/100\n",
      "17103/17103 [==============================] - 15041s 878ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 66/100\n",
      "17103/17103 [==============================] - 15066s 880ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 67/100\n",
      "17103/17103 [==============================] - 15082s 881ms/step - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 68/100\n",
      "17103/17103 [==============================] - 15070s 880ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 69/100\n",
      "17103/17103 [==============================] - 15181s 886ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 70/100\n",
      "17103/17103 [==============================] - 15159s 885ms/step - loss: 0.6932 - accuracy: 0.5006 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 71/100\n",
      "17103/17103 [==============================] - 15188s 887ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 72/100\n",
      "17103/17103 [==============================] - 15195s 887ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 73/100\n",
      "17103/17103 [==============================] - 15152s 885ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 74/100\n",
      "17103/17103 [==============================] - 15158s 885ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 75/100\n",
      "17103/17103 [==============================] - 15174s 886ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 76/100\n",
      "17103/17103 [==============================] - 15194s 887ms/step - loss: 0.6932 - accuracy: 0.5005 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 77/100\n",
      "17103/17103 [==============================] - 15199s 887ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 78/100\n",
      "17103/17103 [==============================] - 15196s 887ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 79/100\n",
      "17103/17103 [==============================] - 15148s 884ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.4996\n",
      "Epoch 80/100\n",
      " 9899/17103 [================>.............] - ETA: 1:25:31 - loss: 0.6932 - accuracy: 0.5012"
     ]
    }
   ],
   "source": [
    "# Define data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "# Build the CNN model with increased complexity and reduced regularization\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                           input_shape=(128, 128, 1)),  #\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with a potentially adjusted learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model.\n",
    "def augment(image, label):\n",
    "    return data_augmentation(image), label\n",
    "\n",
    "augmented_train_ds = train_ds.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "history = model.fit(\n",
    "    augmented_train_ds,\n",
    "    epochs=100,\n",
    "    validation_data=test_ds\n",
    ")\n",
    "\n",
    "#learning rate, batch size, more complex architecture\n",
    "#-> how to overcome underfitting problem\n",
    "#look for opensource commercial products for AI detecting -> to compare accuracy/precision/f1score etc\n",
    "# Use rich texture code to train first on their dataset, then if accruacy matches, train with our dataset\n",
    "\n",
    "# Evaluate the model on the test dataset.\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Take one batch from the test dataset.\n",
    "for images, labels in test_ds.take(1):\n",
    "    predictions = model.predict(images)\n",
    "    predicted_labels = (predictions > 0.5).astype(np.int32).flatten()\n",
    "    true_labels = labels.numpy()\n",
    "\n",
    "    # Display a few images with their true and predicted labels.\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    num_images = min(9, images.shape[0])\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(np.squeeze(images[i].numpy()), cmap='gray')\n",
    "        plt.title(f\"True: {true_labels[i]}, Pred: {predicted_labels[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history.\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIFvUS0zkfvE"
   },
   "source": [
    "Evaluate and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lo17hAGOrZ8v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HMS_DXxjrtx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
